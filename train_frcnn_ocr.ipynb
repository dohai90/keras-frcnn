{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import pprint\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "\n",
    "from keras_frcnn import config, data_generators\n",
    "from keras_frcnn import losses as lossesFns\n",
    "from keras_frcnn import roi_helpers\n",
    "from keras.utils import generic_utils\n",
    "from keras_frcnn.pascal_voc_parser import get_data\n",
    "from keras_frcnn import vgg as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = config.Config()\n",
    "C.use_horizontal_flips = False\n",
    "C.model_path = './weights/model_ocr.h5'\n",
    "C.num_rois = 32\n",
    "C.network = 'vgg'\n",
    "C.base_net_weights = os.path.join('/content/dohai90/pretrained_models', nn.get_weight_path())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_count = dict()\n",
    "class_mapping = dict()\n",
    "classes_count['obj'] = 0\n",
    "classes_count['bg'] = 0\n",
    "class_mapping['obj'] = 0\n",
    "class_mapping['bg'] = len(class_mapping)\n",
    "C.class_mapping = class_mapping\n",
    "\n",
    "config_output_filename = './config_ocr.pickle'\n",
    "with open(config_output_filename, 'wb') as f_in:\n",
    "    pickle.dump(C, f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(start_epoch, stop_epoch):    \n",
    "    words_dir = '/content/dohai90/workspace/OCR/korean_ocr/wordlists'\n",
    "    monogram_file = os.path.join(words_dir, 'wordlist_mono_clean.txt')\n",
    "    bigram_file = os.path.join(words_dir, 'wordlist_bi_clean.txt')\n",
    "    if start_epoch == 0:\n",
    "        string_list = data_generators.build_word_list(16000, monogram_file, bigram_file, max_string_len=4, mono_fraction=1.0)\n",
    "        data_gen_train_ocr = data_generators.get_anchor_gt_ocr(string_list, 1000, 600, C, nn.get_img_output_length, K.image_dim_ordering(), mode='train', multi_fonts=True, multi_font_sizes=True)\n",
    "    else:\n",
    "        string_list = data_generators.build_word_list(32000, monogram_file, bigram_file, max_string_len=16, mono_fraction=0.5)\n",
    "        data_gen_train_ocr = data_generators.get_anchor_gt_ocr(string_list, 1000, 600, C, nn.get_img_output_length, K.image_dim_ordering(), mode='train', multi_fonts=True, multi_font_sizes=True)\n",
    "    \n",
    "    # creating architecture\n",
    "    input_shape_img = (None, None, 3)\n",
    "    img_input = Input(shape=input_shape_img)\n",
    "    roi_input = Input(shape=(None, 4))\n",
    "\n",
    "    # define the base network\n",
    "    shared_layers = nn.nn_base(img_input, trainable=True)\n",
    "\n",
    "    # define the RPN, built on the base network\n",
    "    num_anchors = len(C.anchor_box_scales) * len(C.anchor_box_ratios)\n",
    "    rpn = nn.rpn(shared_layers, num_anchors)\n",
    "    classifier = nn.classifier(shared_layers, roi_input, C.num_rois, nb_classes=len(classes_count), trainable=True)\n",
    "\n",
    "    model_rpn = Model(img_input, rpn[:2])\n",
    "    model_classifier = Model([img_input, roi_input], classifier)\n",
    "\n",
    "    # this is a model that holds both the RPN and the classifier, used to load/save weights for the models\n",
    "    model_all = Model([img_input, roi_input], rpn[:2] + classifier)\n",
    "    \n",
    "    # load weights\n",
    "    if start_epoch == 0:\n",
    "        try:\n",
    "            print('loading weights from {}'.format(C.base_net_weights))\n",
    "            model_rpn.load_weights(C.base_net_weights, by_name=True)\n",
    "            model_classifier.load_weights(C.base_net_weights, by_name=True)\n",
    "        except:\n",
    "            print('Could not load pretrained weights')\n",
    "    else:\n",
    "        try:\n",
    "            print('loading weights from {}'.format(C.model_path))\n",
    "            model_rpn.load_weights(C.model_path, by_name=True)\n",
    "            model_classifier.load_weights(C.model_path, by_name=True)\n",
    "        except:\n",
    "            print('Could not load pretrained weights')\n",
    "            \n",
    "    optimizer = Adam(lr=1e-5)\n",
    "    optimizer_classifier = Adam(lr=1e-5)    \n",
    "    model_rpn.compile(optimizer=optimizer, loss=[lossesFns.rpn_loss_cls(num_anchors), lossesFns.rpn_loss_regr(num_anchors)])\n",
    "    model_classifier.compile(optimizer=optimizer_classifier, loss=[lossesFns.class_loss_cls, lossesFns.class_loss_regr(len(classes_count)-1)], metrics={'dense_class_{}'.format(len(classes_count)): 'accuracy'})\n",
    "    model_all.compile(optimizer='sgd', loss='mae')\n",
    "    \n",
    "    epoch_length = 1000    \n",
    "    iter_num = 0\n",
    "\n",
    "    losses = np.zeros((epoch_length, 5))\n",
    "    best_loss = np.Inf\n",
    "    rpn_accuracy_rpn_monitor = []\n",
    "    rpn_accuracy_for_epoch = []\n",
    "    start_time = time.time()       \n",
    "\n",
    "    for epoch_num in range(start_epoch, stop_epoch):\n",
    "        progbar = generic_utils.Progbar(epoch_length)\n",
    "        print('Epoch {}/{}'.format(epoch_num + 1, stop_epoch))\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                if len(rpn_accuracy_rpn_monitor) == epoch_length and C.verbose:\n",
    "                    mean_overlapping_bboxes = float(sum(rpn_accuracy_rpn_monitor)) / len(rpn_accuracy_rpn_monitor)\n",
    "                    rpn_accuracy_rpn_monitor = []\n",
    "                    print('Average number of overlapping bounding boxes from RPN = {} for {} previous iterations'.format(mean_overlapping_bboxes, epoch_length))\n",
    "                    if mean_overlapping_bboxes == 0:\n",
    "                        print('RPN is not producing boxes that overlap the ground truth boxes. Check RPN settings or keep training')\n",
    "\n",
    "                X, Y, img_data = next(data_gen_train_ocr)\n",
    "\n",
    "                loss_rpn = model_rpn.train_on_batch(X, Y)\n",
    "                P_rpn = model_rpn.predict_on_batch(X)\n",
    "                R = roi_helpers.rpn_to_roi(P_rpn[0], P_rpn[1], C, K.image_dim_ordering(), use_regr=True, overlap_thresh=0.7, max_boxes=300)\n",
    "                # note: calc_iou converts from (x1, y1, x2, y2) to (x, y, w, h) format\n",
    "                X2, Y1, Y2, IoUs = roi_helpers.calc_iou(R, img_data, C, class_mapping)\n",
    "\n",
    "                if X2 is None:\n",
    "                    rpn_accuracy_rpn_monitor.append(0)\n",
    "                    rpn_accuracy_for_epoch.append(0)\n",
    "                    continue\n",
    "\n",
    "                neg_samples = np.where(Y1[0, :, -1] == 1)\n",
    "                pos_samples = np.where(Y1[0, :, -1] == 0)\n",
    "\n",
    "                if len(neg_samples) > 0:\n",
    "                    neg_samples = neg_samples[0]\n",
    "                else:\n",
    "                    neg_samples = []\n",
    "\n",
    "                if len(pos_samples) > 0:\n",
    "                    pos_samples = pos_samples[0]\n",
    "                else:\n",
    "                    pos_samples = []\n",
    "\n",
    "\n",
    "                rpn_accuracy_rpn_monitor.append(len(pos_samples))\n",
    "                rpn_accuracy_for_epoch.append(len(pos_samples))\n",
    "\n",
    "                if C.num_rois > 1:\n",
    "                    if len(pos_samples) < C.num_rois // 2:\n",
    "                        selected_pos_samples = pos_samples.tolist()\n",
    "                    else:\n",
    "                        selected_pos_samples = np.random.choice(pos_samples, C.num_rois // 2, replace=False).tolist()\n",
    "                    try:\n",
    "                        selected_neg_samples = np.random.choice(neg_samples, C.num_rois - len(selected_pos_samples), replace=False).tolist()\n",
    "                    except:\n",
    "                        selected_neg_samples = np.random.choice(neg_samples, C.num_rois - len(selected_pos_samples), replace=True).tolist()\n",
    "\n",
    "                    sel_samples = selected_pos_samples + selected_neg_samples\n",
    "\n",
    "                else:\n",
    "                    # in the extreme case where num_rois = 1, we pick a random pos or neg sample\n",
    "                    selected_pos_samples = pos_samples.tolist()\n",
    "                    selected_neg_samples = neg_samples.tolist()\n",
    "                    if np.random.randint(0, 2):\n",
    "                        sel_samples = random.choice(neg_samples)\n",
    "                    else:\n",
    "                        sel_samples = random.choice(pos_samples)\n",
    "\n",
    "                loss_class = model_classifier.train_on_batch([X, X2[:, sel_samples, :]], [Y1[:, sel_samples, :], Y2[:, sel_samples, :]])\n",
    "\n",
    "                losses[iter_num, 0] = loss_rpn[1]\n",
    "                losses[iter_num, 1] = loss_rpn[2]\n",
    "                losses[iter_num, 2] = loss_class[1]\n",
    "                losses[iter_num, 3] = loss_class[2]\n",
    "                losses[iter_num, 4] = loss_class[3]\n",
    "\n",
    "                iter_num += 1\n",
    "                progbar.update(iter_num, [('rpn_cls', np.mean(losses[:iter_num, 0])), ('rpn_regr', np.mean(losses[:iter_num, 1])),\n",
    "                                          ('detector_cls', np.mean(losses[:iter_num, 2])), ('detector_regr', np.mean(losses[:iter_num, 3]))])\n",
    "\n",
    "                if iter_num == epoch_length:\n",
    "                    loss_rpn_cls = np.mean(losses[:, 0])\n",
    "                    loss_rpn_regr = np.mean(losses[:, 1])\n",
    "                    loss_class_cls = np.mean(losses[:, 2])\n",
    "                    loss_class_regr = np.mean(losses[:, 3])\n",
    "                    class_acc = np.mean(losses[:, 4])\n",
    "\n",
    "                    mean_overlapping_bboxes = float(sum(rpn_accuracy_for_epoch)) / len(rpn_accuracy_for_epoch)\n",
    "                    rpn_accuracy_for_epoch = []\n",
    "\n",
    "                    if C.verbose:\n",
    "                        print('Mean number of bounding boxes from RPN overlapping ground truth boxes: {}'.format(mean_overlapping_bboxes))\n",
    "                        print('Classifier accuracy for bounding boxes from RPN: {}'.format(class_acc))\n",
    "                        print('Loss RPN classifier: {}'.format(loss_rpn_cls))\n",
    "                        print('Loss RPN regression: {}'.format(loss_rpn_regr))\n",
    "                        print('Loss Detector classifier: {}'.format(loss_class_cls))\n",
    "                        print('Loss Detector regression: {}'.format(loss_class_regr))\n",
    "                        print('Elappsed time: {}'.format(time.time() - start_time))\n",
    "\n",
    "                    curr_loss = loss_rpn_cls + loss_rpn_regr + loss_class_cls + loss_class_regr\n",
    "                    iter_num = 0\n",
    "                    start_time = time.time()\n",
    "\n",
    "                    if curr_loss < best_loss:\n",
    "                        if C.verbose:\n",
    "                            print('Total loss decreased from {} to {}, saving weights'.format(best_loss, curr_loss))\n",
    "                        best_loss = curr_loss\n",
    "                        model_all.save_weights(C.model_path)\n",
    "\n",
    "                    break # break out while loop\n",
    "\n",
    "            except Exception as e:\n",
    "                print('Exception: {}'.format(e))\n",
    "                continue   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Start training')\n",
    "train(start_epoch=0, stop_epoch=1)\n",
    "train(start_epoch=1, stop_epoch=2)\n",
    "print('Training done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
